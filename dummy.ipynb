{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d23108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw merged shape: (59526, 120)\n",
      "Spike-in genes detected: 97\n",
      "After removing spike-ins: (59429, 120)\n",
      "Genes zero everywhere: 14289\n",
      "After removing all-zero genes: (45140, 120)\n"
     ]
    }
   ],
   "source": [
    "# cell 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Load count matrix using TPM\n",
    "data_path = '/Users/esrataner/Documents/DATA1030/count_matrix_120'\n",
    "file_list = [f for f in os.listdir(data_path) if f.endswith('.tsv')]\n",
    "\n",
    "def read_tpm_file(filename):\n",
    "    df = pd.read_csv(os.path.join(data_path, filename), sep='\\t',\n",
    "                     usecols=['gene_id', 'TPM'])\n",
    "    df = df.set_index('gene_id')\n",
    "    sample_id = filename.replace('.tsv', '')\n",
    "    df.columns = [sample_id]\n",
    "    return df\n",
    "\n",
    "dfs = [read_tpm_file(f) for f in file_list]\n",
    "merged = pd.concat(dfs, axis=1)  # genes × samples\n",
    "print(\"Raw merged shape:\", merged.shape)\n",
    "\n",
    "# 2. remove all spike-in genes \n",
    "# these are technical control genes - not needed for ML \n",
    "spikein_mask = merged.index.str.lower().str.contains(\n",
    "    \"spikein|ercc\"\n",
    ")\n",
    "\n",
    "print(\"Spike-in genes detected:\", spikein_mask.sum())\n",
    "\n",
    "merged = merged.loc[~spikein_mask]\n",
    "print(\"After removing spike-ins:\", merged.shape)\n",
    "\n",
    "# 3. Remove all-zero genes\n",
    "zero_across_all = (merged == 0).all(axis=1)\n",
    "print(\"Genes zero everywhere:\", zero_across_all.sum())\n",
    "\n",
    "expr = merged[~zero_across_all]\n",
    "print(\"After removing all-zero genes:\", expr.shape)\n",
    "\n",
    "# 4. log1p(TPM) transform — biological normalization\n",
    "expr_log = np.log1p(expr)\n",
    "\n",
    "# NOTE: try graphing log1p and log \n",
    "# depends on if data is close to zero or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12566fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded metadata shape: (1220, 5)\n"
     ]
    }
   ],
   "source": [
    "# cell 2\n",
    "\n",
    "meta = pd.read_csv(\n",
    "    '/Users/esrataner/Documents/DATA1030/tsv/experiment_report_2025_120.tsv',\n",
    "    sep='\\t', skiprows=1\n",
    ")\n",
    "\n",
    "# extract sex\n",
    "meta['Sex'] = meta['Biosample summary'].str.extract(r'(?i)\\b(female|male)\\b')\n",
    "meta['Sex'] = meta['Sex'].str.lower()\n",
    "\n",
    "# parse numeric age\n",
    "def parse_age(a):\n",
    "    if pd.isna(a):\n",
    "        return None\n",
    "    nums = re.findall(r'\\d+', str(a))\n",
    "    return int(nums[0]) if nums else None\n",
    "\n",
    "meta['Age'] = meta['Biosample age'].apply(parse_age)\n",
    "\n",
    "# Age bins\n",
    "def age_bin(x):\n",
    "    if pd.isna(x): return None\n",
    "    if x < 65: return \"60-64\"\n",
    "    if x < 70: return \"65-69\"\n",
    "    if x < 75: return \"70-74\"\n",
    "    if x < 80: return \"75-79\"\n",
    "    if x < 85: return \"80-84\"\n",
    "    if x < 90: return \"85-89\"\n",
    "    return \"90+\"\n",
    "\n",
    "meta['Age_Ordinal'] = meta['Age'].apply(age_bin)\n",
    "\n",
    "age_order = [\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80-84\",\"85-89\",\"90+\"]\n",
    "meta['Age_Ordinal'] = pd.Categorical(\n",
    "    meta['Age_Ordinal'], categories=age_order, ordered=True\n",
    ")\n",
    "\n",
    "# FIX!!  Extract *all* ENCFF file IDs (not just first)\n",
    "# Find ALL ENCFF IDs\n",
    "meta['File_IDs'] = meta['Files'].str.findall(r'ENCFF\\w+')\n",
    "\n",
    "# Expand metadata so each row contains ONE ENCFF ID\n",
    "meta_expanded = meta.explode('File_IDs')\n",
    "\n",
    "# Keep only relevant fields\n",
    "meta_expanded = meta_expanded[\n",
    "    ['Accession', 'Sex', 'Age_Ordinal', 'Biosample accession', 'File_IDs']\n",
    "]\n",
    "\n",
    "# Drop rows missing file IDs\n",
    "meta_expanded = meta_expanded.dropna(subset=['File_IDs'])\n",
    "\n",
    "print(\"Expanded metadata shape:\", meta_expanded.shape)\n",
    "# Expanded metadata shape: (1220, 5)\n",
    "\n",
    "# exp \n",
    "# in the raw ENCODE metadata, each biological sample (ENCSR) has multiple ENCFF files \n",
    "# (technical sequencing files: BAMs, fastqs, quantifications, etc)\n",
    "\n",
    "# After expanding, u get 1 row per ENCFF file\n",
    "# so if each ENCSR has ~10 ENCFF files ->  120 biological samples become ~1220 rows.\n",
    "# so 1220 is NOT 120 samples - it is technical files, NOT samples\n",
    "\n",
    "# MOST IMPORTANTLY - do NOT need GroupShuffleSplit\n",
    "# 1. each ENCSR is represented only once\n",
    "# 2. no ENCFF technical replicates remain#\n",
    "# 3. df_full has 1 row per biological sample\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953d8779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched metadata rows: (120, 5)\n",
      "Replicate sample accessions: []\n",
      "Expression shape after removing replicates: (45140, 120)\n",
      "Metadata shape after removing replicates: (120, 5)\n",
      "Final merged dataset shape: (120, 45143)\n",
      "             13023     26893  30031  30958     30964  ENSG00000000003.14  \\\n",
      "Accession                                                                  \n",
      "ENCSR800PJQ    0.0  0.506818    0.0    0.0  0.000000            1.348073   \n",
      "ENCSR133PLR    0.0  0.000000    0.0    0.0  2.324347            1.340250   \n",
      "ENCSR418WMG    0.0  0.000000    0.0    0.0  0.000000            1.181727   \n",
      "ENCSR013HWB    0.0  0.438255    0.0    0.0  0.000000            1.313724   \n",
      "ENCSR693KOP    0.0  0.000000    0.0    0.0  0.000000            1.081805   \n",
      "\n",
      "             ENSG00000000005.5  ENSG00000000419.12  ENSG00000000457.13  \\\n",
      "Accession                                                                \n",
      "ENCSR800PJQ           0.000000            2.157559            1.444563   \n",
      "ENCSR133PLR           0.122218            2.150599            1.479329   \n",
      "ENCSR418WMG           0.000000            2.149434            1.545433   \n",
      "ENCSR013HWB           0.029559            1.873339            1.095273   \n",
      "ENCSR693KOP           0.000000            2.119863            1.105257   \n",
      "\n",
      "             ENSG00000000460.16  ...  ENSG00000285986.1  ENSG00000285987.1  \\\n",
      "Accession                        ...                                         \n",
      "ENCSR800PJQ            1.297463  ...                0.0           0.029559   \n",
      "ENCSR133PLR            1.068153  ...                0.0           0.067659   \n",
      "ENCSR418WMG            1.108563  ...                0.0           0.000000   \n",
      "ENCSR013HWB            1.111858  ...                0.0           0.104360   \n",
      "ENCSR693KOP            0.916291  ...                0.0           0.019803   \n",
      "\n",
      "             ENSG00000285988.1  ENSG00000285990.1  ENSG00000285991.1  \\\n",
      "Accession                                                              \n",
      "ENCSR800PJQ                0.0           0.000000           0.165514   \n",
      "ENCSR133PLR                0.0           0.000000           0.182322   \n",
      "ENCSR418WMG                0.0           0.364643           0.067659   \n",
      "ENCSR013HWB                0.0           0.000000           0.215111   \n",
      "ENCSR693KOP                0.0           0.000000           0.058269   \n",
      "\n",
      "             ENSG00000285993.1  ENSG00000285994.1     Sex  Age_Ordinal  \\\n",
      "Accession                                                                \n",
      "ENCSR800PJQ                0.0           0.806476  female          90+   \n",
      "ENCSR133PLR                0.0           0.770108    male        85-89   \n",
      "ENCSR418WMG                0.0           0.732368    male        85-89   \n",
      "ENCSR013HWB                0.0           0.647103  female        85-89   \n",
      "ENCSR693KOP                0.0           0.470004  female        85-89   \n",
      "\n",
      "             Biosample accession  \n",
      "Accession                         \n",
      "ENCSR800PJQ          ENCBS159MAA  \n",
      "ENCSR133PLR          ENCBS634VGA  \n",
      "ENCSR418WMG          ENCBS589OUP  \n",
      "ENCSR013HWB          ENCBS046CJD  \n",
      "ENCSR693KOP          ENCBS048EXC  \n",
      "\n",
      "[5 rows x 45143 columns]\n",
      "Dataset Splits:\n",
      "Train: (84, 45141) Target: (84,)\n",
      "Val:   (18, 45141) Target: (18,)\n",
      "Test:  (18, 45141) Target: (18,)\n"
     ]
    }
   ],
   "source": [
    "# cell 3 \n",
    "\n",
    "# List of ENCFF IDs found in expression matrix\n",
    "expr_samples = expr_log.columns.tolist()\n",
    "\n",
    "# Match metadata rows where File_IDs map to expression samples\n",
    "meta_matched = meta_expanded[meta_expanded['File_IDs'].isin(expr_samples)].copy()\n",
    "print(\"Matched metadata rows:\", meta_matched.shape)\n",
    "\n",
    "# Step 1: Relabel expression matrix columns (ENCFF → ENCSR)\n",
    "rename_dict = dict(zip(meta_matched['File_IDs'], meta_matched['Accession']))\n",
    "expr_labeled = expr_log.rename(columns=rename_dict)\n",
    "\n",
    "# Step 2: Identify replicate sample IDs (appear twice)\n",
    "replicate_accessions = expr_labeled.columns[\n",
    "    expr_labeled.columns.duplicated()\n",
    "].unique()\n",
    "\n",
    "print(\"Replicate sample accessions:\", replicate_accessions.tolist())\n",
    "\n",
    "# Step 3: DROP the replicate sample completely\n",
    "expr_no_reps = expr_labeled.drop(columns=list(replicate_accessions))\n",
    "meta_no_reps = meta_matched[\n",
    "    ~meta_matched['Accession'].isin(replicate_accessions)\n",
    "].copy()\n",
    "\n",
    "print(\"Expression shape after removing replicates:\", expr_no_reps.shape)\n",
    "print(\"Metadata shape after removing replicates:\", meta_no_reps.shape)\n",
    "\n",
    "# Step 4: Align expression with metadata\n",
    "expr_T = expr_no_reps.T\n",
    "expr_T.index.name = 'Accession'\n",
    "expr_T = expr_T.reset_index()\n",
    "\n",
    "# Deduplicate final metadata\n",
    "meta_unique = meta_no_reps.drop_duplicates(subset=['Accession'])\n",
    "\n",
    "# Step 5: Merge into final ML-ready dataset\n",
    "df_full = expr_T.merge(\n",
    "    meta_unique.drop(columns=['File_IDs']),\n",
    "    on='Accession',\n",
    "    how='inner'\n",
    ").set_index('Accession')\n",
    "\n",
    "print(\"Final merged dataset shape:\", df_full.shape)\n",
    "print(df_full.head())\n",
    "# cell 4 \n",
    "\n",
    "# TRAIN / VAL / TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (1) Define features + target\n",
    "X = df_full.drop(columns=[\"Sex\", \"Biosample accession\"]) #string carried over \n",
    "y = df_full[\"Sex\"].map({\"female\": 0, \"male\": 1}).astype(int)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "# (2) First split: Train vs Temp (Val+Test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# (3) Split Temp into Validation + Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Print shapes\n",
    "print(\"Dataset Splits:\")\n",
    "print(\"Train:\", X_train.shape, \"Target:\", y_train.shape)\n",
    "print(\"Val:  \", X_val.shape,   \"Target:\", y_val.shape)\n",
    "print(\"Test: \", X_test.shape,  \"Target:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e46049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Splits:\n",
      "Train: (84, 45141) Target: (84,)\n",
      "Val:   (18, 45141) Target: (18,)\n",
      "Test:  (18, 45141) Target: (18,)\n"
     ]
    }
   ],
   "source": [
    "# cell 4 \n",
    "\n",
    "# TRAIN / VAL / TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (1) Define features + target\n",
    "X = df_full.drop(columns=[\"Sex\", \"Biosample accession\"]) #string carried over \n",
    "y = df_full[\"Sex\"].map({\"female\": 0, \"male\": 1}).astype(int)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "# (2) First split: Train vs Temp (Val+Test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# (3) Split Temp into Validation + Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Print shapes\n",
    "print(\"Dataset Splits:\")\n",
    "print(\"Train:\", X_train.shape, \"Target:\", y_train.shape)\n",
    "print(\"Val:  \", X_val.shape,   \"Target:\", y_val.shape)\n",
    "print(\"Test: \", X_test.shape,  \"Target:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb80596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline PR-AUC: 0.22916666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "y_dummy_prob = dummy.predict_proba(X_test)[:, 1]\n",
    "baseline_pr_auc = average_precision_score(y_test, y_dummy_prob)\n",
    "\n",
    "print(\"Baseline PR-AUC:\", baseline_pr_auc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
